\chapter{Matrix Vector Multiplication}
\section{OpenMP Matrix Vector Multiplication}
\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/false-sharing-variable-procs-1000x100000.processed}\falsesharingvarprocsA
\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/tiling-variable-procs-1000x100000.processed}\tilingvarprocsA

\subsection{False Sharing}

The tiling of this loop is based on the length of the input vector.
All parallel instances share the same n in this case, this leads to cache update traffic or invalidates memory reads on the cache level when the result vector \texttt{y[i]} gets updated.

\begin{lstlisting}
  for(uint i=rank; i<n; i+=p) {
    y[i]=0;
    for(uint j=0; j<m; j++) {
      y[i] += x[i][j] * a[j];
    }
  }
\end{lstlisting}

As seen in the plots this leads to a negative performance impact compared to the other implementation.
This is because the the data needs do be redistributed over the cache network everytime an update on a n depending variable happens.

\subsection{Tiling}

The tiling of the loop is based on a section per thread.
In this case all parallel instances only iterate over ``their'' partition. No updates on the cache level between the instances happen and the memory stays consistent.

\begin{lstlisting}
  for(uint i=rank*n/p; i< (rank + 1)*n/p; i++) {
    y[i] = 0;
    for(uint j=0; j<m; j++) {
      y[i] += x[i][j] * a[j];
    }
  }
\end{lstlisting}

The speedup in our test cases  stagnates from 12 threads on in both implementations.
We presume this to the used the 12-core Opteron processors and the messaging overhead between the physical processors.


The correctness of both implementations is tested after each computation.
Every matrix vector product gets calculated in a single thread as a reference and then compared with the multithreaded result.
Also test data is generated with a stable dataset that is generated after the memory of the data structures have been allocated and is always the same with the parameters.
The inputs are sanitized and gracefully exits on wrong inputs with an usage message.

\subsection{Testing}
We tested the algorithms in three different ways to show the improvements.
Static 1000x100000 and 100000x1000 matrices where used to compute the product for \texttt{p = 1..48} procs.
To also show the improvements in relation to the size of matrix and the vector we used a static N and varied the size M over 48 procs we also did the same with a static M and varied the N over 48 procs.



\subsection{Goals, Outcome}

We tried to show the potential speedup of the parallelizable algorithm for matrix vector multiplications.
The resulting data clearly shows the expected speedup although we could not find a worst/bestcase scenario in our tests.


\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
	      xlabel={Threads},
	      ylabel={Time [s]},
	      %xmode=log, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      reverse legend,
	      ymin=0,
	    ]
	    \addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\tilingvarprocsA} \closedcycle;
	    \addplot [stack plots=y, fill=green, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\tilingvarprocsA} \closedcycle;
	    \addplot [stack plots=false, draw=black, thick]					table [x=id, y=med]		{\tilingvarprocsA};
	    \addplot [stack plots=y, fill=red!50, fill opacity=0.4, draw opacity=0, area legend]table [x=id, y expr=\thisrow{max}-\thisrow{med}]	{\tilingvarprocsA} \closedcycle;

	    \addlegendentry{{below med.}}
	    \addlegendentry{{med.}}
	    \addlegendentry{{above med.}}

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing \textbf{run time per number of threads} of the \textbf{Tiling Algorithm} in \textbf{Open-MP} for a \textbf{1000x100000 matrix}. \autoref{res-tbl:3-tilingvarprocsA}}
  \label{plot:3-tilingvarprocsA}
\end{figure}


\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
	      xlabel={Threads},
	      ylabel={Time [s]},
	      %xmode=log, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      reverse legend,
	      ymin=0,
	    ]
	    \addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\falsesharingvarprocsA} \closedcycle;
	    \addplot [stack plots=y, fill=green, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\falsesharingvarprocsA} \closedcycle;
	    \addplot [stack plots=false, draw=black, thick]					table [x=id, y=med]		{\falsesharingvarprocsA};
	    \addplot [stack plots=y, fill=red!50, fill opacity=0.4, draw opacity=0, area legend]table [x=id, y expr=\thisrow{max}-\thisrow{med}]	{\falsesharingvarprocsA} \closedcycle;

	    \addlegendentry{{below med.}}
	    \addlegendentry{{med.}}
	    \addlegendentry{{above med.}}

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing \textbf{run time per number of threads} of the \textbf{False Sharing Algorithm} in \textbf{Open-MP} for a \textbf{1000x100000 matrix}. \autoref{res-tbl:3-tilingvarprocsA}}
  \label{plot:3-falsesharingvarprocsA}
\end{figure}



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={Threads},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=linear,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    %\addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\openmpprefixseqn} \closedcycle;
	    %\addplot [stack plots=y, fill=olive!40!green!60, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\openmpprefixseqn} \closedcycle;
	    \addplot [fill=none, draw=blue, thick]	table [x=id, y=min]		{\falsesharingvarprocsA};
	    \addlegendentry{{False Sharing. minimum}}
	    \addplot [draw=blue, loosely dotted, thick]	table [x=id, y=med]		{\falsesharingvarprocsA};
	    \addlegendentry{{False Sharing. median}}
	    \addplot [fill=none, draw=red, thick]	table [x=id, y=min]		{\tilingvarprocsA};
	    \addlegendentry{{Tiling. minimum}}
	    \addplot [draw=red, loosely dotted, thick]		table [x=id, y=med]		{\tilingvarprocsA};
	    \addlegendentry{{Tiling. median}}
	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the different algorithms in \textbf{OpenMP} for a \textbf{1000x100000 matrix} \textbf{per number of threads}. \autoref{res-tbl:1-1-seq-n} \autoref{res-tbl:1-1-auxarr-n} \autoref{res-tbl:1-1-inplace-n}}
  \label{plot:1-1-n}
\end{figure}


%%% 100000x1000

\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/false-sharing-variable-procs-100000x1000.processed}\falsesharingvarprocsB
\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/tiling-variable-procs-100000x1000.processed}\tilingvarprocsB


\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
	      xlabel={Threads},
	      ylabel={Time [s]},
	      %xmode=log, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      reverse legend,
	      ymin=0,
	    ]
	    \addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\tilingvarprocsB} \closedcycle;
	    \addplot [stack plots=y, fill=green, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\tilingvarprocsB} \closedcycle;
	    \addplot [stack plots=false, draw=black, thick]					table [x=id, y=med]		{\tilingvarprocsB};
	    \addplot [stack plots=y, fill=red!50, fill opacity=0.4, draw opacity=0, area legend]table [x=id, y expr=\thisrow{max}-\thisrow{med}]	{\tilingvarprocsB} \closedcycle;

	    \addlegendentry{{below med.}}
	    \addlegendentry{{med.}}
	    \addlegendentry{{above med.}}

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing \textbf{run time per number of threads} of the \textbf{Tiling Algorithm} in \textbf{Open-MP} for a \textbf{100000x1000 matrix}. \autoref{res-tbl:3-tilingvarprocsB}}
  \label{plot:3-tilingvarprocsB}
\end{figure}


\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
	      xlabel={Threads},
	      ylabel={Time [s]},
	      %xmode=log, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      reverse legend,
	      ymin=0,
	    ]
	    \addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\falsesharingvarprocsB} \closedcycle;
	    \addplot [stack plots=y, fill=green, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\falsesharingvarprocsB} \closedcycle;
	    \addplot [stack plots=false, draw=black, thick]					table [x=id, y=med]		{\falsesharingvarprocsB};
	    \addplot [stack plots=y, fill=red!50, fill opacity=0.4, draw opacity=0, area legend]table [x=id, y expr=\thisrow{max}-\thisrow{med}]	{\falsesharingvarprocsB} \closedcycle;

	    \addlegendentry{{below med.}}
	    \addlegendentry{{med.}}
	    \addlegendentry{{above med.}}

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing \textbf{run time per number of threads} of the \textbf{False Sharing Algorithm} in \textbf{Open-MP} for a \textbf{100000x1000 matrix}.  \autoref{res-tbl:3-tilingvarprocsB}}
  \label{plot:3-falsesharingvarprocsB}
\end{figure}



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={Threads},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=linear,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    %\addplot [stack plots=y, fill=none, draw=none, forget plot]		table [x=id, y=min]		{\openmpprefixseqn} \closedcycle;
	    %\addplot [stack plots=y, fill=olive!40!green!60, fill opacity=0.4, draw opacity=0, area legend]	table [x=id, y expr=\thisrow{med}-\thisrow{min}]	{\openmpprefixseqn} \closedcycle;
	    \addplot [fill=none, loosely dotted, draw=blue, thick]	table [x=id, y=min]		{\falsesharingvarprocsB};
	    \addlegendentry{{False Sharing. minimum}}

      \addplot [draw=blue, thick]	table [x=id, y=med]		{\falsesharingvarprocsB};
	    \addlegendentry{{False Sharing. median}}

      \addplot [fill=none, loosely dotted, draw=red, thick]	table [x=id, y=min]		{\tilingvarprocsB};
	    \addlegendentry{{Tiling. minimum}}

	    \addplot [draw=red, thick]		table [x=id, y=med]		{\tilingvarprocsB};
	    \addlegendentry{{Tiling. median}}
	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the different algorithms in \textbf{OpenMP}  for a \textbf{100000x1000 matrix} per \textbf{number of threads}. \autoref{res-tbl:3-tilingvarprocsB} \autoref{res-tbl:3-falsesharingvarprocsB}}
  \label{plot:1-1-n}
\end{figure}



%%%%%%%%%% VARIABLE N


\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/tiling-fixed-m-100000.processed}\falsesharingFixedMA
\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/false-sharing-fixed-m-100000.processed}\tilingFixedMA



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={N},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=linear,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    \addplot [fill=none, draw=blue, loosely dotted, thick]	table [x=id, y=min]		{\falsesharingFixedMA};
	    \addlegendentry{{False Sharing. minimum}}

      \addplot [draw=blue, thick]	table [x=id, y=med]		{\falsesharingFixedMA};
	    \addlegendentry{{False Sharing. median}}

      \addplot [fill=none, draw=blue, dashed, thick]	table [x=id, y=max]		{\falsesharingFixedMA};
	    \addlegendentry{{False Sharing. maximum}}

	    \addplot [fill=none, draw=red, loosely dotted, thick]	table [x=id, y=min]		{\tilingFixedMA};
	    \addlegendentry{{Tiling. minimum}}

	    \addplot [draw=red, thick]		table [x=id, y=med]		{\tilingFixedMA};
	    \addlegendentry{{Tiling. median}}

      \addplot [draw=red, dashed, thick]		table [x=id, y=max]		{\tilingFixedMA};
	    \addlegendentry{{Tiling. maximum}}

	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the different algorithms in \textbf{OpenMP} for a \textbf{fixed M } and a \textbf{variable N ( 10, 100, 500, 1000, 5000, 10000, 50000, 100000)} with \textbf{10 threads on 36 nodes}.  \autoref{res-tbl:3-tilingFixedMA} \autoref{res-tbl:3-falsesharingFixedMA}}
  \label{plot:1-1-n}
\end{figure}


%%%%%%%%%% VARIABLE M



\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/tiling-fixed-n-100000.processed}\falsesharingFixedNA
\pgfplotstableread[format=file,col sep=comma]{../../../data/1-openmp/p2/false-sharing-fixed-n-100000.processed}\tilingFixedNA



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={M},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=linear,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    \addplot [fill=none, draw=blue, loosely dotted, thick]	table [x=id, y=min]		{\falsesharingFixedNA};
	    \addlegendentry{{False Sharing. minimum}}

      \addplot [draw=blue, thick]	table [x=id, y=med]		{\falsesharingFixedNA};
	    \addlegendentry{{False Sharing. median}}

      \addplot [fill=none, draw=blue, dashed, thick]	table [x=id, y=max]		{\falsesharingFixedNA};
	    \addlegendentry{{False Sharing. maximum}}

	    \addplot [fill=none, draw=red,  loosely dotted, thick]	table [x=id, y=min]		{\tilingFixedNA};
	    \addlegendentry{{Tiling. minimum}}

	    \addplot [draw=red, thick]		table [x=id, y=med]		{\tilingFixedNA};
	    \addlegendentry{{Tiling. median}}

      \addplot [draw=red, dashed, thick]		table [x=id, y=max]		{\tilingFixedNA};
	    \addlegendentry{{Tiling. maximum}}

	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the different algorithms in \textbf{OpenMP} for a \textbf{fixed N } and a \textbf{variable N (10, 100, 500, 1000, 5000, 10000, 50000, 100000)} with \textbf{10 threads on 36 nodes}.  \autoref{res-tbl:3-tilingFixedNA} \autoref{res-tbl:3-falsesharingFixedNA}}
  \label{plot:1-1-n}
\end{figure}



\section{NEC MPI Matrix Vector Multiplication}


\subsection{Allgather}


Every process computes the result for a \texttt{partition = N/size} (where  N is the matrix rank) of the matrix locally and distributes the result to all the other processes.
A difficulty here was to find the right partition size and handle the rest that where smaller than the partition size, also figuring out what happens on which process turned out to be not so easy.
Every process had the whole vector to work with and just the partitions of matrix where distributed.


\begin{lstlisting}
  for(int i= (rank * partition);
  i < ((rank + 1 ) * partition) ;
  i++) {
    sendbuff[i] = 0;
    for (int k=0; k<N; k++) {
      sendbuff[i] = sendbuff[i] + matrix[i][k] * vector[k];
    }
  }

  MPI_Allgather(&sendbuff, partition, ATYPE_MPI,
  &recvbuff, partition, ATYPE_MPI,
  MPI_COMM_WORLD);
\end{lstlisting}



\subsection{ReduceScatter}

First we used MPI\_Scatterv to distribute the matrix and the vector over all nodes.
As data structure for the matrix an array of the length \texttt{N*N} was used so the data could be sent easier to the single threads. Using ReduceScatter we first reduced the partial results and then sent the resulting data further to the other threads doing the local computation of the product.

\begin{lstlisting}
  for( int i=0; i < proc_size; i++) {
    recvcounts[i] = N;
  }

  for ( int i = 0 ; i < partition ; ++i ){
    for ( int j = 0 ; j < N ; ++j ){
      temp_result[j] = temp_result[j] + matrix[i*N+j] * vector[i];
    }
  }
  MPI_Reduce_scatter(temp_result, result,
                     recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
\end{lstlisting}

\subsection{Testing}
To test and verify the results of the computation, we precalculated the results in the root thread in advance.
Then we compared the single partitions of the matrix in the root thread with it.
Measuring the computation time was also tricky, using MPI\_Barrier turned out to be not accurate at all.
Using MPI\_Reduce to get the time of slowest/last finishing thread returned more usable results.

\subsection{Problems}

In the local development we could see a minor speedup, although on the \texttt{Jupiter} the speedup was immeasurable.
The computed result is according to our tests correct, it seems that we introduced a bug that keeps the MPI process from fully distributing the matrix over all threads.
Also using more than 10 procs turned out to be a problem, the processes then started silently segafaulting.
This might be connected to the same bug, but we where not able to fix this in time.


\subsection{Outcome}

From the data we gathered we can say, that the two algorithms do not differ that much in speedup and performance.
This outcome needs to be taken with caution, because the results might be inaccurate due the introduced bug.
In the end we used the save space up to 10 procs to gather the results.

\subsection{Testing}

We tested the algorithms in three different ways to show the improvements.
A static 10000 matrix was used to compute the product for \texttt{p = 1..10} procs.
To also show the improvements in relation to the size of matrix we used matrices with variable N over 10 procs.


%%% Fixed PROCS

\pgfplotstableread[format=file,col sep=comma]{../../../data/3-mpi/p3/allgather-fixed-procs-10.processed}\AllgatherFixedProcs
\pgfplotstableread[format=file,col sep=comma]{../../../data/3-mpi/p3/reduce-scatter-fixed-procs-10.processed}\ReduceScatterFixedProcs



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={N},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    \addplot [fill=none, draw=blue, thick]	table [x=id, y=min]		{\AllgatherFixedProcs};
	    \addlegendentry{{MPI\_Allgather min}}
	    \addplot [draw=blue, loosely dotted, thick]	table [x=id, y=med]		{\AllgatherFixedProcs};
	    \addlegendentry{{MPI\_Allgather med}}
      \addplot [fill=none, draw=blue, dashed, thick]	table [x=id, y=max]		{\AllgatherFixedProcs};
	    \addlegendentry{{MPI\_Allgather max}}

	    \addplot [fill=none, draw=red, thick]	table [x=id, y=min]		{\ReduceScatterFixedProcs};
	    \addlegendentry{{MPI\_ReduceScatter min}}
	    \addplot [draw=red, loosely dotted, thick]		table [x=id, y=med]		{\ReduceScatterFixedProcs};
	    \addlegendentry{{MPI\_ReduceScatter med}}
      \addplot [draw=red, dashed, thick]		table [x=id, y=max]		{\ReduceScatterFixedProcs};
	    \addlegendentry{{MPI\_ReduceScatter max}}

	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the \textbf{MPI\_Allgather and MPI\_ReduceScatter} implementations with \textbf{NEC-MPI} for a \textbf{fixed proc size of 10 } and a \textbf{variable N (100, 500, 1000, 5000, 10000)}.  \autoref{res-tbl:3-allgatherFixedProc} \autoref{res-tbl:3-reduceScatterFixedProc}}
  \label{plot:1-1-n}
\end{figure}


%%% VARIABLE PROCS 1..10

\pgfplotstableread[format=file,col sep=comma]{../../../data/3-mpi/p3/allgather-variable-procs-10000.processed}\AllgatherVariableProcs
\pgfplotstableread[format=file,col sep=comma]{../../../data/3-mpi/p3/reduce-scatter-variable-procs-10000.processed}\ReduceScatterVariableProcs



\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	      width=0.8\linewidth,
        xlabel={Procs},
	      ylabel={Time [s]},
	      %legend columns=4,
	      %legend entries={data:,below median,median,above median,
				%task:,below median,median,above median},
	      xmode=linear, ymode=log,
	      enlarge x limits=false,
	      axis on top,
	      legend transposed =true,
	      legend style={
		      draw=none,
		      cells={anchor=west},
		      legend pos=outer north east,
	      },
	      %reverse legend,
	      ymin=0,
	    ]

	    \addplot [fill=none, draw=blue, thick]	table [x=id, y=min]		{\AllgatherVariableProcs};
	    \addlegendentry{{MPI\_Allgather min}}
	    \addplot [draw=blue, loosely dotted, thick]	table [x=id, y=med]		{\AllgatherVariableProcs};
 	    \addlegendentry{{MPI\_Allgather med}}
      \addplot [fill=none, draw=blue, dashed, thick]	table [x=id, y=max]		{\AllgatherVariableProcs};
	    \addlegendentry{{MPI\_Allgather max}}

	    \addplot [fill=none, draw=red, thick]	table [x=id, y=min]		{\ReduceScatterVariableProcs};
	    \addlegendentry{{MPI\_ReduceScatter min}}
	    \addplot [draw=red, loosely dotted, thick]		table [x=id, y=med]		{\ReduceScatterVariableProcs};
	    \addlegendentry{{MPI\_ReduceScatter med}}
      \addplot [draw=red, dashed, thick]		table [x=id, y=max]		{\ReduceScatterVariableProcs};
	    \addlegendentry{{MPI\_ReduceScatter max}}

	    %\resetstackedplots

	  \end{axis}
  \end{tikzpicture}
  \caption{Benchmark results showing the \textbf{MPI\_Allgather and MPI\_ReduceScatter} implementations with \textbf{NEC-MPI} for a \textbf{fixed size N of 10000 } and \textbf{variable procs 1..10}.  \autoref{res-tbl:3-allgatherVariableProc} \autoref{res-tbl:3-reduceScatterVariableProc}}
  \label{plot:1-1-n}
\end{figure}



\chapter{Benchmarking and Testing}

To keep code duplication as minimal as possible we used one central \texttt{util.c} to share methods for generating data, compare test results and other methods that could come in handy.
Thanks to this we could ensure the correctness of the results and input parameters for our functions.


The benchmarks where done using a suite of shell scripts that would call the programs that we need to test and benchmark.
This helped to keep the process consistent and we could find and fix bugs easier.
We generally tried to automate everything to be fast in regenerating the benchmark data.

If not specified otherwise the benchmarks measuring run time per instance size were using the maximum number of threads of the system they were running on.

The benchmark scripts where also used to process the data and create the statistical output we needed for the report.
After the data was processed it was parsed by the latex package pgfplots to plot the graphs.
